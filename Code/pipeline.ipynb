{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "183610c2",
   "metadata": {},
   "source": [
    "### Importing required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24a721af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular Expression\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from google.cloud import storage\n",
    "import logging\n",
    "\n",
    "# For saving and loading machine learning model\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9526cf7c",
   "metadata": {},
   "source": [
    "### Credential: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1312b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Credential:\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"D:\\classifier.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706247e4",
   "metadata": {},
   "source": [
    "### Apache Beam Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cc6e2e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "INFO:apache_beam.internal.gcp.auth:Setting socket default timeout to 60 seconds.\n",
      "INFO:apache_beam.internal.gcp.auth:socket default timeout is 60.0 seconds.\n",
      "INFO:oauth2client.transport:Attempting refresh to obtain initial access_token\n",
      "INFO:oauth2client.client:Refreshing access_token\n",
      "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', 'C:\\\\Users\\\\tohid\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-f888087e-ff2a-4b1a-8acd-fb1366cf5807.json']\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.8 interpreter.\n",
      "INFO:root:Default Python SDK image for environment is apache/beam_python3.8_sdk:2.33.0\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x000002BB6F02EC10> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x000002BB6F02ED30> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x000002BB6F02F1F0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x000002BB6F02F280> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x000002BB6F02F430> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x000002BB6F02F4C0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x000002BB6F02F5E0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x000002BB6F02F670> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x000002BB6F02F700> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x000002BB6F02F790> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x000002BB6F02F9D0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x000002BB6F02F940> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x000002BB6F02FA60> ====================\n",
      "INFO:apache_beam.runners.worker.statecache:Creating state cache with size 100\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x000002BB6F1041F0> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((ref_AppliedPTransform_Writing-to-google-cloud-Write-WriteImpl-DoOnce-Impulse_14)+(ref_AppliedPTransform_Writing-to-google-cloud-Write-WriteImpl-DoOnce-FlatMap-lambda-at-core-py-2968-_15))+(ref_AppliedPTransform_Writing-to-google-cloud-Write-WriteImpl-DoOnce-Map-decode-_17))+(ref_AppliedPTransform_Writing-to-google-cloud-Write-WriteImpl-InitializeWrite_18))+(ref_PCollection_PCollection_8/Write))+(ref_PCollection_PCollection_9/Write)\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Reading-from-google-cloud-Read-Impulse_4)+(ref_AppliedPTransform_Reading-from-google-cloud-Read-Map-lambda-at-iobase-py-898-_5))+(Reading from google cloud/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Reading from google cloud/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_2_split/Write)\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((((ref_PCollection_PCollection_2_split/Read)+(Reading from google cloud/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Applying-text-preprocess_8))+(ref_AppliedPTransform_Applying-model-on-real-time-data_9))+(ref_AppliedPTransform_Writing-to-google-cloud-Write-WriteImpl-WindowInto-WindowIntoFn-_19))+(ref_AppliedPTransform_Writing-to-google-cloud-Write-WriteImpl-WriteBundles_20))+(ref_AppliedPTransform_Writing-to-google-cloud-Write-WriteImpl-Pair_21))+(Writing to google cloud/Write/WriteImpl/GroupByKey/Write)\n",
      "INFO:root:Naive Bayes model model.sav\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((Writing to google cloud/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_Writing-to-google-cloud-Write-WriteImpl-Extract_23))+(ref_PCollection_PCollection_14/Write)\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((ref_PCollection_PCollection_8/Read)+(ref_AppliedPTransform_Writing-to-google-cloud-Write-WriteImpl-PreFinalize_24))+(ref_PCollection_PCollection_15/Write)\n",
      "INFO:apache_beam.io.gcp.gcsio:Starting the size estimation of the input\n",
      "INFO:apache_beam.io.gcp.gcsio:Finished listing 1 files in 0.18424534797668457 seconds.\n",
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (ref_PCollection_PCollection_8/Read)+(ref_AppliedPTransform_Writing-to-google-cloud-Write-WriteImpl-FinalizeWrite_25)\n",
      "INFO:apache_beam.io.gcp.gcsio:Starting the size estimation of the input\n",
      "INFO:apache_beam.io.gcp.gcsio:Finished listing 1 files in 0.16359949111938477 seconds.\n",
      "INFO:apache_beam.io.gcp.gcsio:Starting the size estimation of the input\n",
      "INFO:apache_beam.io.gcp.gcsio:Finished listing 0 files in 0.15328168869018555 seconds.\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.66 seconds.\n"
     ]
    }
   ],
   "source": [
    "stopWordLst = stopwords.words('english')\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def text_process(text):\n",
    "    \"\"\"\n",
    "    input: raw text, \n",
    "    Applying preprocess and transformations including: \n",
    "    lowercase,\n",
    "    keeping only alphabetical chars,\n",
    "    removing stop_words, \n",
    "    taking root by stem of word\n",
    "    \"\"\"\n",
    "    orginalText = text\n",
    "    text = text.lower()\n",
    "    text = re.sub('[^a-z]', ' ', text)\n",
    "    tokenLst = word_tokenize(text)\n",
    "    lst1 = [stemmer.stem(token) for token in tokenLst if token not in stopWordLst]\n",
    "    text = ' '.join(lst1)\n",
    "    dic1 = {'original': orginalText,\n",
    "           'processed': text}\n",
    "    return dic1\n",
    "\n",
    "def finalResult(dic1):\n",
    "    \"\"\"\n",
    "    In the output file, only the prediction for each message will be saved.\n",
    "    For that matter, the required value from the element dictionary will be extracted.\n",
    "    \"\"\"\n",
    "    res = dic1['result']\n",
    "    return res  \n",
    "    \n",
    "def download_model(bucket_name=None, source_blob_name=None, project=None, destination_file_name=None):\n",
    "    storage_client = storage.Client(project)\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "    blob.download_to_filename(destination_file_name)\n",
    "    \n",
    "class PredictSpam(beam.DoFn):\n",
    "    \n",
    "    def __init__(self, project=None, bucket_name=None, model_path=None, destination_name=None):       \n",
    "        \"\"\"\n",
    "        To deploy the pipeline, Google Cloud Platform is used.\n",
    "        In this initialization, the information of project name, bucket name, tarined model\n",
    "        are passed to the class variables.\n",
    "        \"\"\"\n",
    "        self._model = None\n",
    "        self._project = 'text-classifier-89064107'\n",
    "        self._bucket_name = 'ted_test_data'\n",
    "        self._model_path = 'model.sav'\n",
    "        self._destination_name = 'model.sav'\n",
    "    \n",
    "    def setup(self):\n",
    "        \"\"\"This function will download the trained model from google clouds\"\"\"      \n",
    "        \n",
    "        # Loading model from local disk:\n",
    "        #path = os.path.join(os.path.dirname(os.getcwd()),'test_case\\model\\model.sav')\n",
    "        #self._model = pickle.load(open(path, 'rb'))\n",
    "        \n",
    "        # Loading model from Google Cloud:\n",
    "        logging.info(\"Naive Bayes model {}\".format(self._model_path))\n",
    "        download_model(bucket_name=self._bucket_name,\n",
    "                      source_blob_name=self._model_path,\n",
    "                      project=self._project,\n",
    "                      destination_file_name=self._destination_name)\n",
    "        \n",
    "        # unpickle the trained model:\n",
    "        self._model = pickle.load(open(self._destination_name, 'rb'))\n",
    "        \n",
    "    def process(self, element):\n",
    "        \"\"\"This function will apply the trained model on a given message.\n",
    "        The result will be saved in the element dictionary.\n",
    "        The element dictionary includes three keys:\n",
    "        1- original_text,\n",
    "        2- processed_text,\n",
    "        3- prediction\n",
    "        However, for printing purposes, only the prediction will be exported.\n",
    "        \"\"\"        \n",
    "        predicted = self._model.predict([element['processed']])[0]\n",
    "        if(predicted == 0):\n",
    "            element['result'] = 'Normal'\n",
    "        else:\n",
    "            element['result'] = 'Spam'\n",
    "            \n",
    "        res = element['result']\n",
    "        return [res] \n",
    "        \n",
    "        #return [element]\n",
    "\n",
    "inputfile = 'gs://ted_test_data/df_test.csv'\n",
    "outputfile = 'gs://ted_test_data/result'\n",
    "projectName = 'text-classifier-89064107'\n",
    "bucketName = 'ted_test_data'\n",
    "predicted_table_schema = ('original_text:STRING, processed_text:STRING, result:STRING')\n",
    "tableId = 'text-classifier-89064107:classifier_test_case.prediction'\n",
    "datasetId='text-classifier-89064107:classifier_test_case'\n",
    "\n",
    "def run(argv=None):\n",
    "    \n",
    "    p = beam.Pipeline(options=PipelineOptions())\n",
    "    (p\n",
    "     | 'Reading from google cloud' >> beam.io.ReadFromText(inputfile)\n",
    "     | 'Applying text preprocess' >> beam.Map(text_process)\n",
    "     | 'Applying model on real time data' >> beam.ParDo(PredictSpam())\n",
    "     | 'Writing to google cloud' >> beam.io.WriteToText(outputfile,\n",
    "                                             file_name_suffix='.csv',\n",
    "                                             header='result')\n",
    "    )\n",
    "    result = p.run()\n",
    "    result.wait_until_finish()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# The following pipeline will write the results within a predefined dataset in google cloud:\n",
    "with beam.Pipeline() as pipeline:\n",
    "  (\n",
    "      pipeline\n",
    "      | 'Read lines' >> beam.io.ReadFromText(datapath)\n",
    "      | 'Process' >> beam.Map(text_process)\n",
    "      | 'Prdict spam/ham label' >> beam.ParDo(PredictSklearn())\n",
    "      | 'Print output' >> beam.Map(print_row)\n",
    "      | 'Write results' >> beam.io.WriteToText(outputs_prefix)\n",
    "      | 'Write results' >> beam.io.WriteToBigQuery(table=tableId,\n",
    "                                                   schema = predicted_table_schema,\n",
    "                                                   dataset=datasetId,\n",
    "                                                   project=projectName,\n",
    "                                                   batch_size=int(100),\n",
    "                                                   create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "                                                   write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,\n",
    "                                                   method=\"STREAMING_INSERTS\")\n",
    "  )\n",
    "  \n",
    "# To delete and insert data we can use:\n",
    "# beam.io.BigQueryDisposition.WRITE_TRUNCATE\n",
    "\"\"\"\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
